{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vincent/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/vincent/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vincent/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to /home/vincent/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vincent/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.grammar import Nonterminal\n",
    "from nltk.grammar import ProbabilisticProduction\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.parse import RecursiveDescentParser\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import induce_pcfg\n",
    "from nltk import Nonterminal\n",
    "from nltk import PCFG\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk import tree\n",
    "from nltk import Tree\n",
    "from typing import Iterator, List, Tuple, Union\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('punkt')\n",
    "nltk.download('treebank')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the corpus\n",
    "f = open('corpus.txt')\n",
    "raw = f.read()\n",
    "\n",
    "# Remove empty lines\n",
    "raw = re.sub(r\"\\n\\n\", \"\\n\", raw)\n",
    "\n",
    "# Remove number and philosopther\n",
    "corpus = \"\"\n",
    "for line in raw.split(\"\\n\"):\n",
    "    corpus = corpus + \"\\n\" + re.sub(r\"[0-9]+. \", \"\", line).split(\" - \")[0].lower().replace('.', '').replace(',', '').replace(':', '').replace(';', '') + \"  \" \n",
    "#print(corpus)\n",
    "\n",
    "# Normalize\n",
    "normalized = corpus.lower()\n",
    "normalized = re.sub(r\"[^a-zA-Z0-9]\", \" \", normalized)\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(normalized)\n",
    "sentences = corpus.split(\"  \\n\")[1:]\n",
    "#print(sentences)\n",
    "\n",
    "# Tag each word with part of speech\n",
    "tags = pos_tag(words)\n",
    "\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in words]\n",
    "\n",
    "# print normalized corpus to file\n",
    "with open(\"normalized.txt\", \"w\") as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 1042\n",
      "Vocabulary size: 404\n",
      "Sentences: 65\n",
      "Average sentence length: 16\n",
      "5 Most used words:\n",
      "   - the (51)\n",
      "   - to (42)\n",
      "   - is (39)\n",
      "   - you (36)\n",
      "   - it (23)\n",
      "Hapaxes: 289\n"
     ]
    }
   ],
   "source": [
    "# Some basic statistic\n",
    "print(\"Words:\", len(words))\n",
    "print(\"Vocabulary size:\", len(set(lemmed)))\n",
    "print(\"Sentences:\", len(sentences))\n",
    "print(\"Average sentence length:\", int(len(words)/len(sentences)))\n",
    "\n",
    "fdist = nltk.FreqDist(words)\n",
    "\n",
    "# Count and print most used words\n",
    "print(\"5 Most used words:\")\n",
    "for (word, n) in fdist.most_common(5):\n",
    "    print(\"   -\", word, \"(\" + str(n) + \")\")\n",
    "\n",
    "# Count and print amount of hapaxes\n",
    "hapaxes = 0\n",
    "for (word, n) in fdist.items():\n",
    "    if n == 1:\n",
    "        hapaxes = hapaxes + 1\n",
    "print(\"Hapaxes:\", hapaxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract file to annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unannotated.txt', 'w') as f:\n",
    "    for sentence in sentences:\n",
    "        sent_tags = pos_tag(word_tokenize(sentence))\n",
    "        f.write('(S ')\n",
    "        for word, tag in sent_tags:\n",
    "            f.write(f\"({tag} {word}) \")\n",
    "        f.write(\")\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now copy `unannotated.txt` to `annotated.txt` and manually annotate corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Induce a PCFG from annotated corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = []\n",
    "with open('annotated.txt') as f:\n",
    "    for line in f:\n",
    "        tree = Tree.fromstring(line)\n",
    "        trees.append(tree)\n",
    "\n",
    "productions = []\n",
    "for tree in trees:\n",
    "        productions += tree.productions()\n",
    "        \n",
    "grammar = nltk.induce_pcfg(Nonterminal('S'), productions)\n",
    "listgrammar = [str(prod) for prod in grammar.productions()]\n",
    "listgrammar.sort()\n",
    "\n",
    "with open(\"grammar.txt\", \"w\") as f:\n",
    "    psr = []\n",
    "    lex = []\n",
    "    for i in listgrammar:\n",
    "        if \"'\" in i:\n",
    "            lex.append(i)\n",
    "        else:\n",
    "            psr.append(i)\n",
    "    \n",
    "    f.write(\"# This grammar is sorted alphabetically. The starting node is 'S'\\n\\n# PHRASE STRUCTURE RULES:\\n\\n\")\n",
    "    for i in psr:\n",
    "        f.write(i + \"\\n\")\n",
    "    \n",
    "    f.write(\"\\n\\n# LEXICAL RULES:\\n\\n\")\n",
    "    for i in lex:\n",
    "        f.write(i + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Symbol = Union[str, Nonterminal]\n",
    "\n",
    "class Generator(nltk.grammar.PCFG):\n",
    "    def generate(self, n: int) -> Iterator[str]:\n",
    "        for _ in range(n):\n",
    "            yield self._generate_derivation(self.start())\n",
    "\n",
    "    def _generate_derivation(self, nonterminal: Nonterminal) -> str:\n",
    "        sentence: List[str] = []\n",
    "        symbol: Symbol\n",
    "        derivation: str\n",
    "        for symbol in self._reduce_once(nonterminal):\n",
    "            if isinstance(symbol, str):\n",
    "                derivation = symbol\n",
    "            else:\n",
    "                derivation = self._generate_derivation(symbol)\n",
    "            if derivation != \"\":\n",
    "                sentence.append(derivation)\n",
    "        return \" \".join(sentence)\n",
    "\n",
    "    def _reduce_once(self, nonterminal: Nonterminal) -> Tuple[Symbol]:\n",
    "        return self._choose_production_reducing(nonterminal).rhs()\n",
    "\n",
    "    def _choose_production_reducing(\n",
    "        self, nonterminal: Nonterminal\n",
    "    ) -> ProbabilisticProduction:\n",
    "        productions: List[ProbabilisticProduction] = self._lhs_index[nonterminal]\n",
    "        probabilities: List[float] = [production.prob() for production in productions]\n",
    "        return random.choices(productions, weights=probabilities)[0]\n",
    "\n",
    "\n",
    "def generate(n):\n",
    "    listgrammar.insert(0, 'S -> S [0]') # Manipulate a starting node\n",
    "    rawgrammar = \"\\n\".join(listgrammar).replace('$', 'S')\n",
    "    generator = Generator.fromstring(rawgrammar) # $ will cause errors\n",
    "    sentences = []\n",
    "    while len(sentences) < n:\n",
    "        for sentence in generator.generate(1):\n",
    "            if len(sentence.split(' ')) > 8 and len(sentence.split(' ')) < 32:\n",
    "                sentences.append(sentence.capitalize() + '.')\n",
    "    return sentences\n",
    "\n",
    "generated = generate(50)\n",
    "\n",
    "with open('generated.tex', 'w') as f:\n",
    "    f.write('\\begin{enumerate}')\n",
    "    for sentence in generated:\n",
    "        f.write('\\item '  + sentence)\n",
    "\n",
    "    f.write('\\end{enumerate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
